{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ab8a4b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, accuracy_score, precision_recall_fscore_support, ConfusionMatrixDisplay\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support, ConfusionMatrixDisplay\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "from omegaconf import OmegaConf\n",
    "import pandas as pd\n",
    "\n",
    "# Load configuration\n",
    "cfg = OmegaConf.load('../configs/config.yaml')\n",
    "WAV2VEC2_PATH = cfg.model.save_path\n",
    "CNN_PATH = os.path.join(cfg.model.cnn_save_path, 'cnn_model.pth')\n",
    "VAL_DIR = cfg.data.val_dir\n",
    "SAMPLE_RATE = cfg.data.sample_rate\n",
    "MAX_LENGTH = cfg.data.max_length\n",
    "N_MELS = cfg.data.n_mels\n",
    "EMOTIONS = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprise']\n",
    "\n",
    "# CNN Model Definition\n",
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, num_classes=8):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = torch.nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = torch.nn.Linear(32 * 32 * 39, 128)  # Adjust based on input size\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.fc2 = torch.nn.Linear(128, num_classes)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Dataset for Wav2Vec2\n",
    "class RAVDESSDatasetWav2Vec2(Dataset):\n",
    "    def __init__(self, data_dir, processor, max_length=5.0, sample_rate=16000):\n",
    "        self.data_dir = data_dir\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "        self.sample_rate = sample_rate\n",
    "        self.files = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for file in os.listdir(data_dir):\n",
    "            if file.endswith('.wav'):\n",
    "                self.files.append(os.path.join(data_dir, file))\n",
    "                emotion_id = int(file.split('-')[2]) - 1\n",
    "                self.labels.append(emotion_id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.files[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        waveform, sr = torchaudio.load(file_path)\n",
    "        if sr != self.sample_rate:\n",
    "            waveform = torchaudio.transforms.Resample(sr, self.sample_rate)(waveform)\n",
    "        \n",
    "        max_samples = int(self.max_length * self.sample_rate)\n",
    "        if waveform.shape[1] > max_samples:\n",
    "            waveform = waveform[:, :max_samples]\n",
    "        else:\n",
    "            padding = max_samples - waveform.shape[1]\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "        \n",
    "        inputs = self.processor(waveform.squeeze().numpy(), sampling_rate=self.sample_rate, return_tensors='pt', padding=True)\n",
    "        return {\n",
    "            'input_values': inputs.input_values.squeeze(),\n",
    "            'attention_mask': inputs.attention_mask.squeeze(),\n",
    "            'labels': label\n",
    "        }\n",
    "\n",
    "# Dataset for CNN\n",
    "class RAVDESSMelDataset(Dataset):\n",
    "    def __init__(self, data_dir, sample_rate=16000, n_mels=128, max_length=5.0):\n",
    "        self.data_dir = data_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mels = n_mels\n",
    "        self.max_length = max_length\n",
    "        self.files = []\n",
    "        self.labels = []\n",
    "\n",
    "        for file in os.listdir(data_dir):\n",
    "            if file.endswith('.wav'):\n",
    "                self.files.append(os.path.join(data_dir, file))\n",
    "                emotion_id = int(file.split('-')[2]) - 1\n",
    "                self.labels.append(emotion_id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.files[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        signal, sr = librosa.load(file_path, sr=self.sample_rate)\n",
    "        max_samples = int(self.max_length * self.sample_rate)\n",
    "        if len(signal) > max_samples:\n",
    "            signal = signal[:max_samples]\n",
    "        else:\n",
    "            signal = np.pad(signal, (0, max_samples - len(signal)), 'constant')\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(y=signal, sr=self.sample_rate, n_mels=self.n_mels)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "        mel_spec_db = torch.tensor(mel_spec_db, dtype=torch.float32).unsqueeze(0)\n",
    "        return {\n",
    "            'input': mel_spec_db,\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Function to compute metrics\n",
    "def compute_metrics(labels, preds):\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(labels, preds, title, emotions):\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=emotions, yticklabels=emotions)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Load Wav2Vec2 model and processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(WAV2VEC2_PATH)\n",
    "wav2vec2_model = Wav2Vec2ForSequenceClassification.from_pretrained(WAV2VEC2_PATH)\n",
    "wav2vec2_model.eval()\n",
    "\n",
    "# Load CNN model\n",
    "cnn_model = CNN(num_classes=8)\n",
    "cnn_model.load_state_dict(torch.load(CNN_PATH))\n",
    "cnn_model.eval()\n",
    "\n",
    "# Load validation datasets\n",
    "wav2vec2_val_dataset = RAVDESSDatasetWav2Vec2(VAL_DIR, processor, MAX_LENGTH, SAMPLE_RATE)\n",
    "cnn_val_dataset = RAVDESSMelDataset(VAL_DIR, SAMPLE_RATE, N_MELS, MAX_LENGTH)\n",
    "wav2vec2_val_loader = DataLoader(wav2vec2_val_dataset, batch_size=8, shuffle=False)\n",
    "cnn_val_loader = DataLoader(cnn_val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Evaluate Wav2Vec2\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "wav2vec2_model.to(device)\n",
    "wav2vec2_preds, wav2vec2_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in wav2vec2_val_loader:\n",
    "        input_values = batch['input_values'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = wav2vec2_model(input_values, attention_mask=attention_mask)\n",
    "        wav2vec2_preds.extend(outputs.logits.argmax(dim=-1).cpu().numpy())\n",
    "        wav2vec2_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Evaluate CNN\n",
    "cnn_model.to(device)\n",
    "cnn_preds, cnn_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in cnn_val_loader:\n",
    "        inputs = batch['input'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = cnn_model(inputs)\n",
    "        cnn_preds.extend(outputs.argmax(dim=-1).cpu().numpy())\n",
    "        cnn_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute metrics\n",
    "wav2vec2_metrics = compute_metrics(wav2vec2_labels, wav2vec2_preds)\n",
    "cnn_metrics = compute_metrics(cnn_labels, cnn_preds)\n",
    "\n",
    "# Print metrics\n",
    "print(\"Wav2Vec2 Metrics:\")\n",
    "for metric, value in wav2vec2_metrics.items():\n",
    "    print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "print(\"\\nCNN Metrics:\")\n",
    "for metric, value in cnn_metrics.items():\n",
    "    print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "# Plot confusion matrices\n",
    "plot_confusion_matrix(wav2vec2_labels, wav2vec2_preds, 'Wav2Vec2 Confusion Matrix', EMOTIONS)\n",
    "plot_confusion_matrix(cnn_labels, cnn_preds, 'CNN Confusion Matrix', EMOTIONS)\n",
    "\n",
    "# Compare models\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Wav2Vec2': wav2vec2_metrics,\n",
    "    'CNN': cnn_metrics\n",
    "})\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(metrics_df)\n",
    "\n",
    "# Visualize comparison\n",
    "metrics_df.plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2bf5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f9aafb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segundo_entorno",
   "language": "python",
   "name": "nombre_del_nuevo_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
